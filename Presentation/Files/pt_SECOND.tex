\section{Analysis}
	\subsection{Theoretical}
	
	\subsection{Empirical}

		\begin{frame}{Baselines}
		\textbf{Algorithms used for comparisons}

		% we report here the baselines used to compare the case study algorithm, for clarity
		\begin{itemize}
			\item[$\bullet$] \textbf{balanced quantum k-means} $O(N^2kd)$ (case study) 
			\item[$\bullet$] \textbf{balanced classical k-means} $O(N^3)$ (authors implementation of Malinen et al.)
			\item[$\bullet$] \textbf{classical k-means} $O(Nkd)$ (scikit-learn implementation Lloyd's algorithm)
		\end{itemize}

		classical k-means \textbf{valid comparison} (thanks to dataset structure)

		% altough not balanced, the classical version of the k-means (non balanced) 
		% is still a valid comparison, as we will see in the next slides, 
		% since, due to the balanced structure of the dataset should produce a similar
		% result w.r.t. the balanced algorithms
		
		\end{frame}

		\begin{frame}{Performance Metrics}

			% the performance metrics used for comparison are the ARI and the total computing time
									
			\textbf{Adjusted Rand Index (ARI)}
			\begin{itemize}
				\item[$\bullet$] compare the similarity of two partitions of a dataset 
				\item[$\bullet$] used to compare \textbf{ground truth labels} vs \textbf{clustering algorithm partition} 
				\item[$\bullet$] range from $-1$ to $1$ 
				\item[$\bullet$] ARI = 0 represent a random assignment
			\end{itemize}
			
			\textbf{Total Computing time in quantum approach}
			\begin{equation}
				t = t_{QUBO_{convertion}} + t_{e} + t_{a} + t_{postprocessing}
			\end{equation}

			% here we have 2 metrics for the comparison, the ARI is and indicator
			% of similarity between 2 partitions, it ranges between -1 and 1 
			% high values indicates high similarity, 0 means random assignment
			% and in the paper is used to compare the target partition (correct solution)
			% with the one obtained by the cluster
			% the second metric is the tot computing time, this metric in quantum approach
			% is fragmented in different time measures:
			% $t_{QUBO convertion}$ time to convert the problem in QUBO
			% $t_{e}$ time to embed the QUBO on hardware 	
			% $t_{a}$ time to solve the QUBO (anealing time)
			% $t_{postprocessing}$ extract clustering from binary solution
		
		\end{frame}

		\begin{frame}{Data Generation}
			Synthetic datasets created with \textit{make\_classification} (Scikit-learn)	\\
			\textbf{Datasets structure}
			\begin{itemize}
				\item[$\bullet$] \textbf{N} points	
				\item[$\bullet$] \textbf{k} classes	
				\item[$\bullet$] \textbf{d} features 
				\item[$\bullet$] each clusters \textbf{centered} on one of the \textbf{vertices} of a \textit{d}-dimensional hypercube  
				\item[$\bullet$] points generated from a \textbf{normal dist.} about their cluster center 
				\item[$\bullet$] $\frac{N}{k}$ \textbf{exactly points} per \textbf{class}   
			\end{itemize}

			% data for experiments is generated from a scikit learn function called make_classification
			% for each experiments, as we will see later 50 datasets are created with the following characteristics
			% * description of the list *
			
		\end{frame}

		\begin{frame}{Experiments' Hardware Configuration}	
			\textbf{Classical Machine}
			\begin{itemize}
				\item[$\bullet$] 2.7 GHz Dual-Core Intel i5 
				\item[$\bullet$] 8 GB 1.867 MHz DDR3 memory 
			\end{itemize}

			\textbf{Quantum Machine}
			\begin{itemize}
				\item[$\bullet$] D-Wave 2000Q quantum computer 
				\item[$\bullet$] 2048 qubits, 5600 inter-qubit connections
			\end{itemize}

			\textbf{Technical Aspects}
			\begin{itemize}
				\item[$\bullet$] \textbf{quantum pre/post-processing} done via \textbf{classical} machine
				\item[$\bullet$] quantum \textbf{anealing} operation \textbf{perfomed 100 times} for each experiment   
			\end{itemize}			

			% the Hardware used for the experiments is reported in this slide
			% * description of the list *

		\end{frame}

		\begin{frame}{Experiments}
			\textbf{Quality of Clustering Experiment (ARI)}
			% this experiment aims to compare the clustering quality of each algorithms

			\begin{itemize}
				\item[$\bullet$] \textbf{clustering quality} of the 3 algorithms is compared
				\item[$\bullet$] each algorithm evaluated on different \textbf{problem types}
				% a problem type is defined by a points and clusters, we have a total of 9 problem types
				\begin{itemize}
					\item[$\circ$] total of 9 problem types 
					\item[$\circ$] defined by \textit{(num. of points, num. of clusters)}
					\item[$\circ$] e.g. \textit{(16 points, 2 clusters)}
				\end{itemize}

				\item[$\bullet$] for each problem type:
				\begin{itemize}
					\item[$\circ$] all the 3 algorithm evaluated on 50 \textbf{synthetic datasets}
					\item[$\circ$] \textbf{Averaged ARI} is reported  
				\end{itemize}
			\end{itemize}

			% the experiments are carried for different problem types, problem type are
			% defined by the number of points and number of clusters for instance (12, 3)
			
		\end{frame}

		\begin{frame}{Experiments}
			\textbf{Commenting Results for Quantum Approach}

			\begin{table}[]
			\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}|c|c|@{}}
			\toprule
			\textbf{observation}                                                                                                  & \textbf{possible cause/motivation}                                                                                           \\ \midrule
			\begin{tabular}[c]{@{}c@{}}drop in performance for \\ $k=2$ classical vs quantum\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}less way to cluster, local solution is \\ more likely to be the correct one\end{tabular} \\ \midrule
			quantum performances drop as the problem size increase                                                                & reflection of the quantum hardware                                                                                  \\ \midrule
			\begin{tabular}[c]{@{}c@{}}best quantum performances obtained \\ for problem types (8,4) (12, 3) (12, 4)\end{tabular} & \begin{tabular}[c]{@{}c@{}}as quantum computer improves author's \\ approach may outperform classical\end{tabular}  \\ \bottomrule
			\end{tabular}%
			}
			\end{table}

			\begin{center}
				\includegraphics[scale=0.65]{first_ARI_table.png}
			\end{center}

			% * same things in the slides *
		\end{frame}

		\begin{frame}[allowframebreaks]{Limitations and Approximations}
			\textbf{Limitations faced}
			\begin{itemize}
				\item[$\bullet$] \textbf{Variable limitation} D-Wave 2000Q qubit limitation for problems $Nk > 64$ var.
				\item[$\bullet$] \textbf{Qubit connectivity} "limitation" $=>$ higher embedding time
			\end{itemize}
			\textbf{Approximations}
			\begin{itemize}
				\item[$\bullet$] Quantum run time for larger problems ($Nk > 64$)
				\begin{itemize}
					\item[$\circ$] used to evaluate scalability of the Quantum Approach
					\item[$\circ$] measure $t_{QUBO_{convertion}}$ and $t_{postprocessing}$ (measurable)
					\item[$\circ$] estimate embedding time $t_{e}$ (\textbf{extrapolated} from smaller problems)
					\item[$\circ$] estimate annealing time $t_{a}$ (constant, \textbf{averaging} smaller problems) 
				\end{itemize} 
			\end{itemize}
			$t_e$ \textbf{scale quadratically} in the number of \textbf{binary variables} of the QUBO 
			\begin{equation}
				t_{e} = 1.887 \times 10^{-6}(Nk)^2 
				+ 4.632 \times 10^{-6}(Nk)
				+ 4.022 \times 10^{-4}
			\end{equation}
			\begin{equation}
				t_{a} = 0.03481 \pm 0.00008
			\end{equation}

			% here the main limitations are about variables, since D-Wave 2000Q can't handle problems with more than 64 variables
			% and the qubit connectivity, of course the more the connections the more time is required by the embedding algorithm
			% approximation introduced is the evaluations for problems with more than 64 variables, since we cannot test them in
			% practice, to estimate the new computing time we have to approximate the anealing time and the embedding time, other 
			% values are measurable, since they happen in classical. since the embedding algorithm choosed by authors scale quadratically
			% in num of var can be approx with this formula. for what concern the anealing time, since it is costant one measured
			% in previous examples is used
		\end{frame}

		\begin{frame}{Scalability}
			\textbf{Experiments to assess scalability}
			\begin{itemize}
				\item[$\bullet$] evaluation metric used is \textbf{average total computing} time (\textbf{estimated} for quantum)
				\item[$\bullet$] baselines evaluated on the \textbf{three variables}:
				\begin{itemize}
					\item[$\circ$] $N$ data points  					
					\item[$\circ$] $k$ clusters
					\item[$\circ$] $d$ features 
				\end{itemize}
				\item[$\bullet$] baselines runned on 50 \textbf{synthetic datasets}
			\end{itemize}

			% * same things in the slides *
		\end{frame}

		\begin{frame}[allowframebreaks]{Scalability w.r.t. points (N)}
			\textbf{Details}
			\begin{itemize}
				\item[$\bullet$] baselines evaluated on increasing \textbf{data points}
				\item[$\bullet$] fixed cluster $k=4$ and features $d=2$
			\end{itemize}

			% embedding time difficult due to limited qubit connectivity	

			\textbf{Considerations}
			\begin{table}[]
			\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}|c|c|@{}}
			\toprule
			\textbf{observation}                                 & \textbf{motivation/improvement} \\ \midrule
			\textbf{quantum approach slower} than both classical & \begin{tabular}[c]{@{}c@{}}dominated by embedding time, \\ in future can improve for N\textgreater{}1024\end{tabular} \\ \midrule
			classical k-means scale the best                     & $O(Nkd)$ vs $O(N^3)$ and $O(N^2kd)$                                                                                                                                     \\ \bottomrule
			\end{tabular}%
			}
			\end{table}

			\begin{center}
				\includegraphics[scale=0.35]{scalability_points.png}
			\end{center}	
			
		\end{frame}

		\begin{frame}[allowframebreaks]{Scalability w.r.t. clusters (k)}
			\textbf{Details}
			\begin{itemize}
				\item[$\bullet$] baselines evaluated on increasing \textbf{cluster size}
				\item[$\bullet$] fixed data points $N=256$ and features $d=8$
			\end{itemize} 

			\textbf{Considerations}
			\begin{table}[]
			\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}|c|c|@{}}
			\toprule
			\textbf{observation} & \textbf{motivation/improvement}\\ \midrule
			\begin{tabular}[c]{@{}c@{}}quantum longer \textbf{time} on k\\ \textbf{scale worse} as k increase\end{tabular} & \begin{tabular}[c]{@{}c@{}}Third term in the QUBO formulation\\ time complexity $O(Nk^2)$\end{tabular} \\ \bottomrule
			\end{tabular}%
			}
			\end{table}

			\begin{center}
				\includegraphics[scale=0.35]{scalability_clusters.png}
			\end{center}
		\end{frame}

		\begin{frame}[allowframebreaks]{Scalability w.r.t. features (d)}
			\small
			\textbf{Details}
			\begin{itemize}
				\item[$\bullet$] baselines evaluated on increasing \textbf{features number}
				\item[$\bullet$] fixed data points $N=1024$ and cluster $k=4$ 
			\end{itemize}
			%\QUBO formulation only requires one comput. related to the dimension of the dataset 
			% QUBO formulation only requires one comput. of the distance matrix 
			% this is because the quantum approach compute the variance though the total variance explained by matteo  
			%classical k-means recomputes distances from centroids at each iteration 
			% since recompute centroids at each iterations

			\textbf{Considerations}
			\begin{table}[]
			\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}|c|c|@{}}
			\toprule
			\textbf{observation}                                                                                                           & \textbf{motivations/improvements}                                                                                                                                          \\ \midrule
			quantum had longest time                                                                                                       & \begin{tabular}[c]{@{}c@{}}\textbf{improvements} in hardware and embeddings could outperform\\ classical k-means $d>128$\\ classical balanced k-means $d<256$\end{tabular} \\ \midrule
			\begin{tabular}[c]{@{}c@{}}quantum scales better w.r.t. classical k-means \\ as $d$ increases\end{tabular}                     & \begin{tabular}[c]{@{}c@{}}QUBO formulation only requires one comput. of the distance matrix\\ classical re-compute distance from centroids at each iteration\end{tabular} \\ \midrule
			\begin{tabular}[c]{@{}c@{}}\textit{classical balanced k-means} scales \\ better in $d$ w.r.t. to quantum approach\end{tabular} & $O(N^2kd)$ vs $O(N^3)$                                                                                                                                                      \\ \bottomrule
			\end{tabular}%
			}
			\end{table}

			\begin{center}
				\includegraphics[scale=0.35]{scalability_features.png}
			\end{center}
			% * same things in the slides *
		\end{frame}
	
	\subsection{Benchmark}
	\begin{frame}[allowframebreaks]{Clustering a Benchmark Data Set}
		\textbf{The Iris Dataset}
		\begin{itemize}
			\item[$\bullet$] Reduced due to qubit limitations on modern hardware
			\item[$\bullet$] Pick $N/k$ points from $2\leq k \leq3$ of the data set's classes
		\end{itemize}
	
		\textbf{Experiments Run}
		\begin{itemize}
			\item[$\bullet$] All the 3 clustering algorithms were tested
			\item[$\bullet$] Experiments are run on 50 subsets of the dataset
		\end{itemize}
	
		\textbf{Results}
		\begin{itemize}
			\item[$\bullet$] $k=2$
			\begin{itemize}
				\item[$\circ$] Trivial case, points are linearly separable
				\item[$\circ$] Classical algorithms perform better than quantum
				\item[$\circ$] Evident as the number of binary variables $(Nk)$ increases
			\end{itemize}
			% Iris dataset used as proof of consistency of the previous results on a typical benchmarck dataset
			% Even if the dataset is composed of just 150 points with 4 features and k classes the quantum algorithm is still not powerful enough
			% Hence they reduced the number of points to N/k and considered k between 2 and 3
			% Results for k = 2 are significantly better for the classical algorithms since points are linearly separable and the global optimum is always found
		
			\framebreak
			\item[$\bullet$] $k=3$
			\begin{itemize}
				\item[$\circ$] Similar performance to \textbf{classical balanced} k-means
				\item[$\circ$] Outperforms \textbf{Scikit-Learn} implementation
				\item[$\circ$] Performance of the QA degrades as the problem size increases
			\end{itemize}
		\end{itemize}
		\begin{center}
			\includegraphics[scale=0.45]{Iris_ARI_table.png}
		\end{center}
		% For k = 3 the quantum algorithm performs as well as the classical balanced k-means and outperforms Scikit-learn implementation
		% The ARI table for this last experiments shows the results obtained and makes evidence that when the size of the problem increases, the performances of the QA degrade
	\end{frame}